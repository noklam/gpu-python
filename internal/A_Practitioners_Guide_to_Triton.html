<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>gpu-python - Why &amp; when to use Triton</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-4KGX64DRBZ"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-4KGX64DRBZ', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">gpu-python</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/noklam/gpu-python" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/noklamchan" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Why &amp; when to use Triton</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<h1>
<b>A practitioner’s guide to Triton</b>
</h1>
<p>By UmerHA (https://x.com/UmerHAdil // https://github.com/UmerHA/), for the cuda-mode group ❤️ May our brrrr level reach over 9000.</p>
<p><strong>What is Triton</strong></p>
<p>In short: Triton is a language to program GPUs more conventiently. You write Python-ish code, which is then compiled into ptx code (the same thing cuda code is compiled into).</p>
<p>During the compilation, the Triton compiler tries to use clever tricks to rearrange the parts of your program (without changing the program’s meaning!) to make it run faster.</p>
<p><strong>Triton vs Cuda</strong></p>
<p><img src="images/1_cuda_v_triton.png"></p>
<p>source: https://zhuanlan.zhihu.com/p/672086654</p>
<p>CUDA is a high-end tool with many settings for the pros. - full control over everything, so absolute max performance possible - harder to get decent performance - way more tedious to write and debug - more complicated, so harder to learn</p>
<p>Triton is a very good tool for most users - you can’t control everything, as some things are left to automatic optimization; so you probably won’t get absolute max performance - way easier to get good performance - way easier to write and debug - easier to learn, as it has a Python-like syntax</p>
<p><strong>Triton vs torch.compile</strong></p>
<p><code>torch.compile</code> makes your model faster by trying to use existing kernels more effectively and creating simple new kernels. This may make your model fast enough. If not, you can decide to invest time to write faster Triton kernels.</p>
<p>(These simple new kernels that <code>torch.compile</code> creates are actually Triton kernels. So they are a good starting point for your custom kernels. See <a href="https://twitter.com/marksaroufim">Mark Saroufim</a>’s <a href="https://www.youtube.com/watch?v=LuhJEEJQgUM&amp;t=2200s">lecture 1 of cuda mode</a> for how.)</p>
<p><strong>When to use Triton</strong></p>
<p>You start with your AI model. 1. If it’s not fast enough, <code>torch.compile</code> it. 2. If it’s not fast enough, check if you can rewrite your code to make it more suitable for <code>torch.compile</code>. 3. If it’s not fast enough, check which parts are slow and write custom Triton kernel(s) for those. 4. If it’s not fast enough, check which parts are slow and write custom CUDA kernel(s) for those.</p>
<p>(In the unlikely case you know beforehand tyou need absolute max performance, you can decide to directly start with CUDA.)</p>
<p><strong>A note on rough edges</strong></p>
<p>As Triton is a newer project, people have found it to have a few rough edges. I have noted all rough edges I encountered with the comment “Weirdness: &lt;description of what’s weird to me&gt;”.</p>
<p>I expect it to get a lot more polished over time.</p>
<section id="how-to-write-triton-kernels" class="level1">
<h1>How to write Triton kernels</h1>
<p>Unlike with CUDA, we can debug Triton kernels just like any CPU program, if we set the environment variable <code>TRITON_INTERPRET = 1</code>. Then Triton runs on the CPU but simulates that it runs on the GPU.</p>
<p>I recommend writing all programs in the simulator first, and checking for correctness. If correct, then you can make it fast.</p>
<p>Below are some utility functions for debugging: - <code>check_tensors_gpu_ready</code>: (i) assert all tensors are contiguous in memory and (ii) only if not simulating, assert all tensors are on gpu - <code>breakpoint_if</code>: set a breakpoint, depending on conditions on pids - <code>print_if</code> print sth, depending on conditions on pids</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.core.debugger <span class="im">import</span> set_trace</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'TRITON_INTERPRET'</span>] <span class="op">=</span> <span class="st">'1'</span> <span class="co"># needs to be set *before* triton is imported</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_tensors_gpu_ready(<span class="op">*</span>tensors):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> tensors:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> t.is_contiguous, <span class="st">"A tensor is not contiguous"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.environ.get(<span class="st">'TRITON_INTERPRET'</span>) <span class="op">==</span> <span class="st">'1'</span>: <span class="cf">assert</span> t.is_cuda, <span class="st">"A tensor is not on cuda"</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_pid_conds(conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Test if condition on pids are fulfilled</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">    E.g.:</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">        '=0'  checks that pid_0 == 0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">        ',&gt;1' checks that pid_1 &gt; 1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        '&gt;1,=0' checks that pid_0 &gt; 1 and pid_1 == 0</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    pids <span class="op">=</span> pid_0[<span class="dv">0</span>], pid_1[<span class="dv">0</span>], pid_2[<span class="dv">0</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    conds <span class="op">=</span> conds.replace(<span class="st">' '</span>,<span class="st">''</span>).split(<span class="st">','</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (cond, pid) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(conds, pids)):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> cond<span class="op">==</span><span class="st">''</span>: <span class="cf">continue</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        op, threshold <span class="op">=</span> cond[<span class="dv">0</span>], <span class="bu">int</span>(cond[<span class="dv">1</span>:])</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> op <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'&lt;'</span>,<span class="st">'&gt;'</span>,<span class="st">'&gt;='</span>,<span class="st">'&lt;='</span>,<span class="st">'='</span>, <span class="st">'!='</span>]: <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Rules may only use these ops: '&lt;','&gt;','&gt;=','&lt;=','=', '!='. Invalid rule: '</span><span class="sc">{</span>condition<span class="sc">}</span><span class="ss">'."</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        op <span class="op">=</span> <span class="st">'=='</span> <span class="cf">if</span> op <span class="op">==</span> <span class="st">'='</span> <span class="cf">else</span> op</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">eval</span>(<span class="ss">f'</span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>op<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span>threshold<span class="sc">}</span><span class="ss">'</span>): <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> test_pid_conds(<span class="st">''</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> test_pid_conds(<span class="st">'&gt;0'</span>, [<span class="dv">1</span>], [<span class="dv">1</span>])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="kw">not</span> test_pid_conds(<span class="st">'&gt;0'</span>, [<span class="dv">0</span>], [<span class="dv">1</span>])</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> test_pid_conds(<span class="st">'=0,=1'</span>, [<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">0</span>])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> breakpoint_if(conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Stop kernel, if any condition of pids is fulfilled'''</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> test_pid_conds(conds, pid_0, pid_1, pid_2): set_trace()</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_if(txt, conds, pid_0<span class="op">=</span>[<span class="dv">0</span>], pid_1<span class="op">=</span>[<span class="dv">0</span>], pid_2<span class="op">=</span>[<span class="dv">0</span>]):</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''Print txt, if any condition of pids is fulfilled'''</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> test_pid_conds(conds, pid_0, pid_1, pid_2): <span class="bu">print</span>(txt)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cdiv(a,b): <span class="cf">return</span> (a <span class="op">+</span> b <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> b</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> cdiv(<span class="dv">10</span>,<span class="dv">2</span>)<span class="op">==</span><span class="dv">5</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> cdiv(<span class="dv">10</span>,<span class="dv">3</span>)<span class="op">==</span><span class="dv">4</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="programming-model" class="level1">
<h1>Programming model</h1>
<p>With CUDA, we decompose the computation in 2 levels: First into blocks, then each block further into threads. All threads in a block run on the same SM and share the same Shared Memory. And each thread computes on <strong>scalars</strong>.</p>
<p>In Triton, we decompose the computation only in 1 level: Into blocks. There is no further decomposition into threads. <strong>Triton requires us to perform operations on vectors</strong>. Also, we don’t need to and are not able to manage the shared memory. Triton does that automatically.</p>
<p>Example:</p>
<p>Let’s say we want to add <code>x</code> and <code>y</code>, which are vectors of size 8, and save the output into <code>z</code> (also size 8). Let’s use blocks of size 4, so we have <code>8 / 4 = 2</code> blocks. - Cuda runs 2 blocks, each with 4 threads. Each of the 8 threads computes a single position, e.g.&nbsp;<code>z[0] = x[0] + y[0]</code> - Triton also runs 2 blocks, which each performs vectorized addition. The vector size is the block size, which is 4. E.g. <code>z[0:3] = x[0:3] + y[0:3]</code></p>
<p><strong>All</strong> operations in triton kernels are vectorized: Loading data, operating on data, storing data, and creating masks.</p>
<p>Let’s think about another simple example:</p>
<p>Again, we want to add <code>x</code> and <code>y</code>, which are now vectors of size <strong>6</strong>, and save the output into <code>z</code> (also size 6). Let’s use blocks of size 4, so we have <code>cdiv(6, 4) = 2</code> blocks.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x, y, x<span class="op">+</span>y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>(tensor([1, 2, 3, 4, 5, 6]),
 tensor([0, 1, 0, 1, 0, 1]),
 tensor([1, 3, 3, 5, 5, 7]))</code></pre>
</div>
</div>
<p>The cuda kernel would be the C-equivalent of this:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># x,y = input tensors, z = output tensors, n = size of x, bs = block size</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_cuda_k(x, y, z, n, bs):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># locate which part of the overall computation this specific kernel is doing</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    block_id <span class="op">=</span> ... <span class="co"># in our example: one of [0,1] </span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    thread_id <span class="op">=</span> ... <span class="co"># in our example: one of [0,1,2,3] </span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># identify the location of the data this specific kernel needs</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> block_id <span class="op">*</span> bs <span class="op">+</span> thread_id</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># guard clause, to make sure we're not going out of bounds</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> offs <span class="op">&lt;</span> n:</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># read data</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        x_value <span class="op">=</span> x[offs]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        y_value <span class="op">=</span> y[offs]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># do operation</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        z_value <span class="op">=</span> x_value <span class="op">+</span> y_value</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># write data</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        z[offs] <span class="op">=</span> z_value</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Important: offs, x_value, y_value, x_value are all scalars!</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The guard condition kind of is also a scalar, as it check one condition on one value.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For illustration, here are the variables for every kernel:</p>
<p><img src="images/2_cuda_variables.png"></p>
<p>Let’s now look at the corresponding triton kernel, which roughly looks like this:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: this is for illustration, and not quite syntactically correct. See further below for correct triton syntax</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> add_triton_k(x, y, z, n, bs):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># locate which part of the overall computation this specific kernel is doing</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    block_id <span class="op">=</span> tl.program_id(<span class="dv">0</span>)  <span class="co"># in our example: one of [0,1] </span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># identify the location of the data this specific kernel needs</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> block_id <span class="op">*</span> bs <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs) <span class="co"># &lt;- this is a vector!</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the guard clause becomes a mask, which is a vector of bools</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n <span class="co"># &lt;- this is a vector of bools!</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># read data</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> x[offs] <span class="co"># &lt;- a vector is read!</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    y_values <span class="op">=</span> y[offs] <span class="co"># &lt;- a vector is read!</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># do operation</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    z_value <span class="op">=</span> x_value <span class="op">+</span> y_value  <span class="co"># &lt;- vectors are added!</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># write data</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    z[offs] <span class="op">=</span> z_value  <span class="co"># &lt;- a vector is written!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Again, for illustration, here are the variables for every kernel:</p>
<p><img src="images/3_triton_variables.png"></p>
<p>Note on jargon: In triton lingo, each kernel (which processes a block) is called a “program”. I.e., our example above runs 2 programs. Therefore, “block_id” is often called “pid” (short for “program id”), but it’s the same.</p>
</section>
<section id="example-1-copying-a-tensor" class="level1">
<h1>Example 1: Copying a tensor</h1>
<p>Let’s looks at some examples. To keeps things simple, we’ll use very small block sizes.</p>
<p>Goal: Given a tensor <code>x</code> of shape (n), copy it into another tensor <code>z</code>.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # This is a normal python function, which launches the triton kernels</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy(x, bs, kernel_fn):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> torch.zeros_like(x)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(x, z)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> x.numel()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    n_blocks <span class="op">=</span> cdiv(n, bs)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> (n_blocks,)  <span class="co"># how many blocks do we have? can be 1d/2d/3d-tuple or function returning 1d/2d/3d-tuple</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># launch grid!</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - kernel_fn is the triton kernel, which we write below</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - grid is the grid we constructed above</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># - x,z,n,bs are paramters that are passed into each kernel function</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    kernel_fn[grid](x,z,n,bs)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Note:</strong> For educational purposes, the kernel below has a logic bug (but the syntax is correct). Can you spot it?</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # This is the triton kernel:</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The triton.jit decorator takes a python function and turns it into a triton kernel, which is run on the GPU.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Inside this function only a subset of all python ops are allowed.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># E.g., when NOT simulating, we can't print or use breakpoints, as these don't exist on the GPU. </span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># When we pass torch tensors, they are automatically converted into a pointer to their first value</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># E.g., above we passed x, but here we receive x_ptr</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> tl.arange(<span class="dv">0</span>, bs)  <span class="co"># compute the offsets from the pid </span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask) <span class="co"># load a vector of values, think of `x_ptr + offs` as `x_ptr[offs]`</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask) <span class="co"># store a vector of values</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Question: What is wrong with this kernel?</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [2] | offs = [0 1], mask = [ True  True], x = [1 2]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([1, 2, 0, 0, 0, 0])</code></pre>
</div>
</div>
<p>We were not shifting the offets correcltly. We always used offsets = [0,1], but they should change with the pid.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> pid <span class="op">*</span> n <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [6 7], mask = [False False], x = [1 1]
pid = [2] | offs = [12 13], mask = [False False], x = [1 1]</code></pre>
</div>
</div>
<p>Not quite correct. We added <code>pid * n</code>, but want to add <code>pid * bs</code></p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    pid <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> pid <span class="op">*</span> bs <span class="op">+</span> tl.arange(<span class="dv">0</span>, bs)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> offs <span class="op">&lt;</span> n</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs, x, mask)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    print_if(<span class="ss">f'pid = </span><span class="sc">{</span>pid<span class="sc">}</span><span class="ss"> | offs = </span><span class="sc">{</span>offs<span class="sc">}</span><span class="ss">, mask = </span><span class="sc">{</span>mask<span class="sc">}</span><span class="ss">, x = </span><span class="sc">{</span>x<span class="sc">}</span><span class="ss">'</span>, <span class="st">''</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> copy(x, bs<span class="op">=</span><span class="dv">2</span>, kernel_fn<span class="op">=</span>copy_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>pid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]
pid = [1] | offs = [2 3], mask = [ True  True], x = [3 4]
pid = [2] | offs = [4 5], mask = [ True  True], x = [5 6]</code></pre>
</div>
</div>
<p>Yes!</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>x, z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>(tensor([1, 2, 3, 4, 5, 6]), tensor([1, 2, 3, 4, 5, 6]))</code></pre>
</div>
</div>
<p>As we saw, writing GPU programs involves many indices, which we can easily mess up. So I highly recommend writing and debugging the kernel in simuation mode, and testing with tiny examples first!</p>
</section>
<section id="example-2-greyscaling-an-image" class="level1">
<h1>Example 2: Greyscaling an image</h1>
<p><em>Restart kernel here</em></p>
<p>In this example, we’ll grayscale an image of a puppy. We’ll see how we can work on 2d data.</p>
<p>This works analogously for 3D data.</p>
<p>We’ve adapted Jeremy Howard’s example from this <a href="https://colab.research.google.com/drive/180uk6frvMBeT4tywhhYXmz3PJaCIA_uk?usp=sharing">colab</a> / <a href="https://www.youtube.com/watch?v=4sgKnKbR-WE&amp;feature=youtu.be">youtube</a>. So, h/t for the example and selection of puppy image.</p>
<p><em>Side note: Two weird things happen in this example, if we don’t restart the kernel:</em> 1. <em>torchvision can’t be imported, probably due to a circular dependency. -&gt; I currently don’t know why, need to dig deeper.</em> 2. <em>the simulated triton kernel below fails, because a float can’t be mutliplied to a uint vector -&gt; Works on GPU w/o simulation, so seems to be a <code>TRITON_INTERPRET</code> bug.</em></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> urllib.request <span class="im">import</span> urlretrieve</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> tensor</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision <span class="im">as</span> tv</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms.functional <span class="im">as</span> tvf</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> io</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cdiv(a,b): <span class="cf">return</span> (a <span class="op">+</span> b <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>path_img <span class="op">=</span> Path(<span class="st">'puppy.jpg'</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> path_img.exists(): urlretrieve(url, path_img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> io.read_image(<span class="st">'puppy.jpg'</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(img.shape)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>img[:<span class="dv">2</span>,:<span class="dv">3</span>,:<span class="dv">4</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([3, 1066, 1600])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>tensor([[[117, 119, 117, 113],
         [119, 129, 129, 113],
         [130, 126, 122, 115]],

        [[ 83,  85,  85,  80],
         [ 85,  97,  97,  82],
         [ 98,  93,  89,  83]]], dtype=torch.uint8)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_img(x, figsize<span class="op">=</span>(<span class="dv">4</span>,<span class="dv">3</span>), <span class="op">**</span>kwargs):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>figsize)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">'off'</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(x.shape)<span class="op">==</span><span class="dv">3</span>: x <span class="op">=</span> x.permute(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)  <span class="co"># CHW -&gt; HWC</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    plt.imshow(x.cpu(), <span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> tvf.resize(img, <span class="dv">150</span>, antialias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>ch,h,w <span class="op">=</span> img.shape</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>ch,h,w,h<span class="op">*</span>w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>(3, 150, 225, 33750)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>show_img(img)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>To work with 2d data, we’ll build 2d offsets and masks. Here’s an illustration how it works, e.g.&nbsp;for an <code>4x7</code> matrix and block sizes of <code>2</code> for each dimensions.</p>
<p><img src="images/4_offset_2d.png"></p>
<p>And in code, it looks like this:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    pid_0 <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    pid_1 <span class="op">=</span> tl.program_id(<span class="dv">1</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    offs_0 <span class="op">=</span> pid_0 <span class="op">*</span> bs0 <span class="op">+</span> tl.arange(<span class="dv">0</span>,bs0)  <span class="co"># 1d vector</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    offs_1 <span class="op">=</span> pid_1 <span class="op">*</span> bs1 <span class="op">+</span> tl.arange(<span class="dv">0</span>,bs1)  <span class="co"># 1d vector</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Weirdness: None-slicing currently doesn't work when simulating on cpu. Use tl.expand_dim instead.</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># offs = w * tl.expand_dims(offs_0, 1) + tl.expand_dims(offs_1, 0)</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> w <span class="op">*</span> offs_0[:,<span class="va">None</span>] <span class="op">+</span> offs_1[<span class="va">None</span>, :]  <span class="co"># 2d matrix! - we multiply first offset by width, see image above</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    mask_0 <span class="op">=</span> offs_0 <span class="op">&lt;</span> h  <span class="co"># 1d vector</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    mask_1 <span class="op">=</span> offs_1 <span class="op">&lt;</span> w  <span class="co"># 1d vector</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mask = tl.expand_dims(mask_0, 1) &amp; tl.expand_dims(mask_1, 0)</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> mask_0[:,<span class="va">None</span>] <span class="op">&amp;</span> mask_1[<span class="va">None</span>,:]  <span class="co"># 2d matrix! - data musn't go out of bounds along either axis, therefore `logical and` of the individual masks</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">0</span><span class="op">*</span>h<span class="op">*</span>w<span class="op">+</span>offs, mask<span class="op">=</span>mask)</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">1</span><span class="op">*</span>h<span class="op">*</span>w<span class="op">+</span>offs, mask<span class="op">=</span>mask)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>h<span class="op">*</span>w<span class="op">+</span>offs, mask<span class="op">=</span>mask)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Weirdness: multiplying float with uint vectors fails when simulating on cpu</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="fl">0.2989</span><span class="op">*</span>r <span class="op">+</span> <span class="fl">0.5870</span><span class="op">*</span>g <span class="op">+</span> <span class="fl">0.1140</span><span class="op">*</span>b  <span class="co"># don't worry why it's these 3 numbers we're multiplying with</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    tl.store(out_ptr <span class="op">+</span> offs, out, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s use the kernel!</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rgb2grey(x, bs):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    c,h,w <span class="op">=</span> x.shape</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> torch.empty((h,w), dtype<span class="op">=</span>x.dtype, device<span class="op">=</span>x.device)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># grid can be a function returning a 1d/2d/3d-tuple</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (having a grid function is not more useful than a grid tuple in this case, but will be below when benchmarking &amp; auto-tuning)</span></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (cdiv(h, meta[<span class="st">'bs0'</span>]), cdiv(w,  meta[<span class="st">'bs1'</span>]))</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    rgb2grey_k[grid](x, out, h, w, bs0<span class="op">=</span>bs[<span class="dv">0</span>], bs1<span class="op">=</span>bs[<span class="dv">1</span>]) <span class="co"># all kwargs are passed into grid function</span></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out.view(h,w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="11">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>grey_img <span class="op">=</span> rgb2grey(img.to(<span class="st">'cuda'</span>), bs<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">32</span>)).to(<span class="st">'cpu'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>show_img(grey_img, cmap<span class="op">=</span><span class="st">'gray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-37-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Very cool</p>
</section>
<section id="example-3-matmul" class="level1">
<h1>Example 3: Matmul</h1>
<p><em>For simplicity, restart kernel here</em></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="co"># os.environ['TRITON_INTERPRET'] = '1'</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> triton.language <span class="im">as</span> tl</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># moved util functions into separate file for better readability</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> triton_util <span class="im">import</span> cdiv, breakpoint_if, print_if, check_tensors_gpu_ready</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s implement a naive matmul in Triton. We’ll learn: - A method to split computation - Calling functions from our kernel - Using pre-implemented vector/matrix ops within an block</p>
<p>This is adapted from the <a href="https://openai.com/research/triton">OpenAI blog post announcing Triton</a>.</p>
<p>We want to multiply the <code>m x k</code>-matrix <code>A</code> and the <code>k x n</code>-matrix <code>B</code> into the <code>m x n</code>-matrix <code>C</code>.</p>
<p>We split the computation along each of the three axes: - along the m axis - we’ll use block dimension 0 to represent this - along the n axis - we’ll use block dimension 1 to represent this - along the shared k axis - this will not be represented by a block. All chunks of computation will be done in same block.</p>
<p><img src="images/5_matmul_split.png"></p>
<p>Because we frequently create 1d- or 2d-offets and -masks, let’s put that functionality into utility functions. As long as these functions are <code>triton.jit</code>-ed, they can be used in the kernel.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_1d_offset(size, n_prev_chunks):</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> n_prev_chunks <span class="op">*</span> size <span class="op">+</span> tl.arange(<span class="dv">0</span>, size)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_2d_offset(offs_0, offs_1, stride_0, stride_1<span class="op">=</span><span class="dv">1</span>): </span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tl.expand_dims(offs_0, <span class="dv">1</span>)<span class="op">*</span>stride_0 <span class="op">+</span> tl.expand_dims(offs_1, <span class="dv">0</span>)<span class="op">*</span>stride_1</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_1d_mask(offs, <span class="bu">max</span>):</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> offs <span class="op">&lt;</span> <span class="bu">max</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_2d_mask(offs_0, offs_1, max_0, max_1):</span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (tl.expand_dims(offs_0, <span class="dv">1</span>) <span class="op">&lt;</span> max_0) <span class="op">&amp;</span> (tl.expand_dims(offs_1, <span class="dv">0</span>) <span class="op">&lt;</span> max_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here’s the naive matmul kernel:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> naive_matmul_k(</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>    m, n, k,</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, </span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn,</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn,</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># chunks along m/n/k dimensions</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># relevant offsets of a, b</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize and iteratively update accumulator</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># todo umer: don't we need mask when loading a &amp; b?</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>) <span class="co"># matmul in block ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># increase offets, so next iteration loads next chunks</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> matmul(a, b, matmul_k_fn, bs<span class="op">=</span><span class="dv">16</span>, group_sz<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> a.shape[<span class="dv">1</span>] <span class="op">==</span> b.shape[<span class="dv">0</span>], <span class="st">"matrix dims not compatible for matmul"</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(a, b)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    (m, k), (_, n) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.empty((m, n), device<span class="op">=</span>a.device, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (triton.cdiv(m, meta[<span class="st">'bm'</span>]),  triton.cdiv(n, meta[<span class="st">'bn'</span>]))</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    group_sz <span class="op">=</span> {} <span class="cf">if</span> group_sz <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> {<span class="st">"group_sz"</span>:group_sz} <span class="co"># not used in naive_matmul, but will be in grouped_matmul further below </span></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn[grid](</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        a, b, c,</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        m, n, k,</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>        a.stride(<span class="dv">0</span>), a.stride(<span class="dv">1</span>),</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>        b.stride(<span class="dv">0</span>), b.stride(<span class="dv">1</span>),</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        c.stride(<span class="dv">0</span>), c.stride(<span class="dv">1</span>),</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        bm<span class="op">=</span>bs, bn<span class="op">=</span>bs, bk<span class="op">=</span>bs, <span class="co"># Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>group_sz</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>naive_matmul <span class="op">=</span> partial(matmul, matmul_k_fn<span class="op">=</span>naive_matmul_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>naive_matmul(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
</div>
</div>
<p>Let’s unit test this against PyTorch’s implementation</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>triton_output <span class="op">=</span> naive_matmul(a, b)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>torch_output <span class="op">=</span> torch.matmul(a, b)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.allclose(triton_output, torch_output, atol<span class="op">=</span><span class="fl">5e-2</span>, rtol<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Triton and Torch match"</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Triton and Torch differ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>✅ Triton and Torch match</code></pre>
</div>
</div>
</section>
<section id="example-4-faster-matmul" class="level1">
<h1>Example 4: Faster Matmul</h1>
<p><em>Note: Needs code from example 3, so run that before</em></p>
<p>Triton handles the order of memory access <strong>within</strong> blocks, but not <strong>across</strong> blocks. So this is a knob we can use to make our kernels faster.</p>
<p>In fact, cleverly reordering blocks can increase L2-cache hit rate, which makes our kernels faster. This example is taken from the <a href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html">triton docs</a>.</p>
<p>Now, to make better use of the L2 cache, we want to reuse data that’s was recently loaded, and is therefore likely still in the L2 cache. How? By reducing the number of <em>different</em> data loads that a bunch of “consecutive” kernels need. By “consecutive” we mean kernels that are executed approximately at the same time.</p>
<p>This picture (adapter from the <a href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html">triton docs</a>) shows how we can do that. If we order naively, the first row of the output matrix is computed “consecutively”, which needs 90 different block reads (9 from matrix A, 81 from matrix B). If we use “group ordering”, a 3x3 square of blocks of the output matrix is computed “consecutively”, which needs 54 different block reads (27 from matrix A, 27 from matrix B).</p>
<p><img src="images/6_matmul_order.png"></p>
<p><em>Note: In the docs, grouping is called “super-grouping”</em></p>
<p>Okay, how can we tell Triton in which order to process blocks? The answer is: We take the pids, change them, and use them as if they were the original pids.</p>
<p>Let’s do a minimal example to illustrate this principle:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_item(<span class="bu">id</span>): <span class="bu">print</span>(<span class="ss">f"I'm processing item </span><span class="sc">{</span><span class="bu">id</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>): process_item(i)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I'm processing item 0
I'm processing item 1
I'm processing item 2
I'm processing item 3
I'm processing item 4</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> change_id(old_id): <span class="cf">return</span> <span class="dv">5</span><span class="op">-</span>old_id</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>): process_item(change_id(i))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>I'm processing item 5
I'm processing item 4
I'm processing item 3
I'm processing item 2
I'm processing item 1</code></pre>
</div>
</div>
<p>Et voilà, the items were processed in a different order.</p>
<p>So how should the pid-change-function for faster matmul look like? It should change the left matrix into the right matrix.</p>
<p><img src="images/7_swizzling_exmple.png"></p>
<p>On the left, the default ordering is shown (called “row-major”). Remember, we deal with blocks. We can’t arrange how the individual cells are processed, only the blocks. In the picture, our output matrix C has <code>5x7 = 35</code> cells, but only <code>cdiv(5,1) x cdiv(7,2) = 5x4 = 20</code> blocks.</p>
<p>On the right, notice how the first 9 processed blocks are the <code>3x3</code> grid we want! We process 3 blocks in a column. Then advance a column, again process 3, advance, and so on. The orange lines show where advance. This operation is called <strong>“swizzling”</strong>.</p>
<p>By the way, you can of course change the number 3. It’s called the <code>group_size</code>.</p>
<p>You don’t need to write swizzling yourself, as there is a <code>triton.language.swizzle2d</code> function.</p>
<p>To really understand <code>swizzle2d</code>, let’s quickly verifiy it works as expected. We’ll then continue to use it in our faster matmul kernel.</p>
<p><em>Side-Goal:</em> Use <code>swizzle2d</code> on a <code>5x4</code> matrix with elements <code>0 ... 19</code> in row-major ordering. We should then get a matrix with elements in grouped ordering.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    num_pid_m, num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">0</span>), tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    pid_m_, pid_n_ <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  <span class="co"># Weirdness: tl.swizzle2d doesn't work when simulating on CPU</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>    offs_m <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    offs_n <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    offs <span class="op">=</span> get_2d_offset(offs_m, offs_n, stride_0<span class="op">=</span>num_pid_n)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(offs_m, offs_n, max_0<span class="op">=</span>num_pid_m, max_1<span class="op">=</span>num_pid_n )</span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>    offs_sw_m <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_m_)</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>    offs_sw_n <span class="op">=</span> get_1d_offset(<span class="dv">1</span>, n_prev_chunks<span class="op">=</span>pid_n_)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    offs_sw <span class="op">=</span> get_2d_offset(offs_sw_m, offs_sw_n, stride_0<span class="op">=</span>num_pid_n)</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    mask_sw <span class="op">=</span> get_2d_mask(offs_sw_m, offs_sw_n, max_0<span class="op">=</span>num_pid_m, max_1<span class="op">=</span>num_pid_n)</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tl.load(x_ptr <span class="op">+</span> offs, mask<span class="op">=</span>mask)</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    tl.store(z_ptr <span class="op">+</span> offs_sw, x, mask<span class="op">=</span>mask_sw)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>blocks_m, blocks_n <span class="op">=</span> <span class="dv">5</span>,<span class="dv">4</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(blocks_m<span class="op">*</span>blocks_n, device<span class="op">=</span><span class="st">'cuda'</span>).view(blocks_m,blocks_n)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]], device='cuda:0')</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> <span class="op">-</span>torch.ones_like(x) <span class="co"># empty matrix, with -1 denoting empty</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor([[-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1],
        [-1, -1, -1, -1]], device='cuda:0')</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># swizzle x into z</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>swizzle_k[(blocks_m,blocks_n)](x,z, group_sz<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>tensor([[ 0,  3,  6,  9],
        [ 1,  4,  7, 10],
        [ 2,  5,  8, 11],
        [12, 14, 16, 18],
        [13, 15, 17, 19]], device='cuda:0')</code></pre>
</div>
</div>
<p>Looks good!</p>
<hr>
<p>Let’s now implement the grouped matmul kernel, which will be faster than the regular matmul.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_matmul_k(</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    m, n, k,</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, </span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn,</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn,</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.program_id(<span class="dv">0</span>), tl.program_id(<span class="dv">1</span>)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>    num_pid_m, num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">0</span>), tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine location of block in grouped ordering - swizzle! </span></span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  <span class="co"># Weirdness: tl.swizzle2d doesn't work when simulating on CPU</span></span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># chunks along m/n/k dimensions</span></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb57-16"><a href="#cb57-16" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb57-17"><a href="#cb57-17" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb57-18"><a href="#cb57-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># relevant offsets of a, b</span></span>
<span id="cb57-19"><a href="#cb57-19" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb57-20"><a href="#cb57-20" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb57-21"><a href="#cb57-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize and iteratively update accumulator</span></span>
<span id="cb57-22"><a href="#cb57-22" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb57-23"><a href="#cb57-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb57-24"><a href="#cb57-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># todo umer: don't we need mask when loading a &amp; b?</span></span>
<span id="cb57-25"><a href="#cb57-25" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)</span>
<span id="cb57-26"><a href="#cb57-26" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)</span>
<span id="cb57-27"><a href="#cb57-27" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>) <span class="co"># block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile</span></span>
<span id="cb57-28"><a href="#cb57-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># increase offets, so next iteration loads next chunks</span></span>
<span id="cb57-29"><a href="#cb57-29" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb57-30"><a href="#cb57-30" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb57-31"><a href="#cb57-31" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb57-32"><a href="#cb57-32" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb57-33"><a href="#cb57-33" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>grouped_matmul <span class="op">=</span> partial(matmul, matmul_k_fn<span class="op">=</span>grouped_matmul_k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.ones((<span class="dv">3</span>, <span class="dv">4</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.ones((<span class="dv">4</span>, <span class="dv">5</span>), dtype<span class="op">=</span>torch.float32, device<span class="op">=</span><span class="st">'cuda'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>grouped_matmul(a,b, group_sz<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
</div>
</div>
<p>Let’s unit test this against PyTorch’s implementation</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn((<span class="dv">512</span>, <span class="dv">512</span>), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>triton_output <span class="op">=</span> grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>torch_output <span class="op">=</span> torch.matmul(a, b)</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.allclose(triton_output, torch_output, atol<span class="op">=</span><span class="fl">5e-2</span>, rtol<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Triton and Torch match"</span>)</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Triton and Torch differ"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>✅ Triton and Torch match</code></pre>
</div>
</div>
</section>
<section id="benchmarking" class="level1">
<h1>Benchmarking</h1>
<p>Triton brings built-in benchmarking tools with it. Here’s an example how to use it.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># adapted from https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>],  <span class="co"># Argument names to use as an x-axis for the plot.</span></span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>        x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)],  <span class="co"># Different possible values for `x_name`.</span></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>        x_log<span class="op">=</span><span class="va">True</span>,  <span class="co"># x axis is logarithmic.</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>,  <span class="co"># Argument name whose value corresponds to a different line in the plot.</span></span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>        line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>],  <span class="co"># Possible values for `line_arg`.</span></span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>        line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],  <span class="co"># Label name for the lines.</span></span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],  <span class="co"># Line styles.</span></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>,  <span class="co"># Label name for the y-axis.</span></span>
<span id="cb64-12"><a href="#cb64-12" aria-hidden="true" tabindex="-1"></a>        plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>,  <span class="co"># Name for the plot. Used also as a file name for saving the plot.</span></span>
<span id="cb64-13"><a href="#cb64-13" aria-hidden="true" tabindex="-1"></a>        args<span class="op">=</span>{},  <span class="co"># Values for function arguments not in `x_names` and `y_name`.</span></span>
<span id="cb64-14"><a href="#cb64-14" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb64-15"><a href="#cb64-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb64-16"><a href="#cb64-16" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb64-17"><a href="#cb64-17" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb64-18"><a href="#cb64-18" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb64-19"><a href="#cb64-19" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb64-20"><a href="#cb64-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: naive_matmul(a, b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb64-21"><a href="#cb64-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>: ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb64-22"><a href="#cb64-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: torch.matmul(a,b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb64-23"><a href="#cb64-23" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb64-24"><a href="#cb64-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-67-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.085106  0.085106  0.053691
1                64.0  0.129730  0.125000  0.107143
2               128.0  0.159468  0.154341  0.170515
3               256.0  0.097909  0.099071  0.125654
4               512.0  0.030346  0.030361  0.111079
5              1024.0  0.006971  0.007279  0.034461
6              2048.0  0.001405  0.001749  0.006355</code></pre>
</div>
</div>
<p><em>Note Umer: I would’ve expected the GB/s to increase as the matrix sizes get larger. Why don’t they? Maybe because share memory is full, so kernel spends more and more time reloading stuff</em></p>
<p>Let’s try different block sizes:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'batch_size'</span>], x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">7</span>, <span class="dv">1</span>)], x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>, line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>], line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>, plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>, args<span class="op">=</span>{}</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(batch_size, provider):</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span>batch_size), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>: ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: grouped_matmul(a, b, bs<span class="op">=</span>batch_size, group_sz<span class="op">=</span><span class="dv">8</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: torch.matmul(a,b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb67-17"><a href="#cb67-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span>
<span id="cb67-18"><a href="#cb67-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-19"><a href="#cb67-19" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-69-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>matmul-performance:
   batch_size     Naive   Grouped     Torch
0        16.0  0.030404  0.030433  0.111111
1        32.0  0.060683  0.061127  0.111111
2        64.0  0.083660  0.084026  0.111111</code></pre>
</div>
</div>
<p>Larger block sizes seem to be better. Let’s compare with pytorch again, using larger block sizes.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>], x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)], x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>, line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'torch'</span>], line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Torch'</span>],</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>, plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>, args<span class="op">=</span>{}</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span><span class="dv">64</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>: ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>, bs<span class="op">=</span><span class="dv">64</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: torch.matmul(a,b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb69-16"><a href="#cb69-16" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb69-17"><a href="#cb69-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span>
<span id="cb69-18"><a href="#cb69-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-19"><a href="#cb69-19" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-70-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>matmul-performance:
   square_matrix_size     Naive   Grouped     Torch
0                32.0  0.039867  0.038710  0.053215
1                64.0  0.077922  0.071006  0.106667
2               128.0  0.109091  0.107143  0.169912
3               256.0  0.137733  0.136364  0.126150
4               512.0  0.084731  0.083916  0.111047
5              1024.0  0.021879  0.025362  0.034691
6              2048.0  0.005257  0.005919  0.007440</code></pre>
</div>
</div>
<p>This reduces the performance difference to pytorch for larger matrix sizes, but pytorch is still better.</p>
<p>Tip: For profiling, we can use Nsight Compute to profile our kernels: <code>ncu --target-processes all your_python_file.py</code></p>
</section>
<section id="auto-tuning" class="level1">
<h1>Auto-Tuning</h1>
<p>Adapted from https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html</p>
<p>The choice of meta-parameters (e.g.&nbsp;block sizes) and compilation options (e.g.&nbsp;<code>num_warps</code>) impacts the kernel speed. Triton allows you to pass a list of possible choices, runs them all, and then compiles the kernel for the fastest choice. This is called <code>Auto-Tuning</code>.</p>
<p>If the size of your problem changes (e.g.&nbsp;when matrix changes size), a new auto-tune will be done for the new problem size.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.autotune</span>(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Choices of configs to auto-tune over</span></span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>    configs<span class="op">=</span>[</span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">256</span>, <span class="st">'bk'</span>: <span class="dv">64</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">3</span>, num_warps<span class="op">=</span><span class="dv">8</span>),</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">256</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">128</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">64</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb71-8"><a href="#cb71-8" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">128</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb71-9"><a href="#cb71-9" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">128</span>, <span class="st">'bn'</span>: <span class="dv">32</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">4</span>, num_warps<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb71-10"><a href="#cb71-10" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">64</span>, <span class="st">'bn'</span>: <span class="dv">32</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">5</span>, num_warps<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb71-11"><a href="#cb71-11" aria-hidden="true" tabindex="-1"></a>        triton.Config({<span class="st">'bm'</span>: <span class="dv">32</span>, <span class="st">'bn'</span>: <span class="dv">64</span>, <span class="st">'bk'</span>: <span class="dv">32</span>, <span class="st">'group_sz'</span>: <span class="dv">8</span>}, num_stages<span class="op">=</span><span class="dv">5</span>, num_warps<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb71-12"><a href="#cb71-12" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb71-13"><a href="#cb71-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Definition of problem size. If it changes, a new auto-tune is run for the new problem size.</span></span>
<span id="cb71-14"><a href="#cb71-14" aria-hidden="true" tabindex="-1"></a>    key<span class="op">=</span>[<span class="st">'m'</span>, <span class="st">'n'</span>, <span class="st">'k'</span>],</span>
<span id="cb71-15"><a href="#cb71-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb71-16"><a href="#cb71-16" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.jit</span></span>
<span id="cb71-17"><a href="#cb71-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_autotuned_matmul_k(</span>
<span id="cb71-18"><a href="#cb71-18" aria-hidden="true" tabindex="-1"></a>    a_ptr, b_ptr, c_ptr,</span>
<span id="cb71-19"><a href="#cb71-19" aria-hidden="true" tabindex="-1"></a>    m, n, k,</span>
<span id="cb71-20"><a href="#cb71-20" aria-hidden="true" tabindex="-1"></a>    stride_am, stride_ak, </span>
<span id="cb71-21"><a href="#cb71-21" aria-hidden="true" tabindex="-1"></a>    stride_bk, stride_bn,</span>
<span id="cb71-22"><a href="#cb71-22" aria-hidden="true" tabindex="-1"></a>    stride_cm, stride_cn,</span>
<span id="cb71-23"><a href="#cb71-23" aria-hidden="true" tabindex="-1"></a>    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr</span>
<span id="cb71-24"><a href="#cb71-24" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb71-25"><a href="#cb71-25" aria-hidden="true" tabindex="-1"></a>    pid_m <span class="op">=</span> tl.program_id(<span class="dv">0</span>)</span>
<span id="cb71-26"><a href="#cb71-26" aria-hidden="true" tabindex="-1"></a>    pid_n <span class="op">=</span> tl.program_id(<span class="dv">1</span>)</span>
<span id="cb71-27"><a href="#cb71-27" aria-hidden="true" tabindex="-1"></a>    num_pid_m <span class="op">=</span> tl.num_programs(<span class="dv">0</span>)</span>
<span id="cb71-28"><a href="#cb71-28" aria-hidden="true" tabindex="-1"></a>    num_pid_n <span class="op">=</span> tl.num_programs(<span class="dv">1</span>)</span>
<span id="cb71-29"><a href="#cb71-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine location of block in grouped ordering</span></span>
<span id="cb71-30"><a href="#cb71-30" aria-hidden="true" tabindex="-1"></a>    pid_m, pid_n <span class="op">=</span> tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  <span class="co"># Weirdness: tl.swizzle2d doesn't work when simulating on CPU</span></span>
<span id="cb71-31"><a href="#cb71-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># chunks along m/n/k dimensions</span></span>
<span id="cb71-32"><a href="#cb71-32" aria-hidden="true" tabindex="-1"></a>    rm <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bm, n_prev_chunks<span class="op">=</span>pid_m)</span>
<span id="cb71-33"><a href="#cb71-33" aria-hidden="true" tabindex="-1"></a>    rn <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bn, n_prev_chunks<span class="op">=</span>pid_n)</span>
<span id="cb71-34"><a href="#cb71-34" aria-hidden="true" tabindex="-1"></a>    rk <span class="op">=</span> get_1d_offset(size<span class="op">=</span>bk, n_prev_chunks<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb71-35"><a href="#cb71-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># relevant offsets of a, b</span></span>
<span id="cb71-36"><a href="#cb71-36" aria-hidden="true" tabindex="-1"></a>    offs_a <span class="op">=</span> a_ptr <span class="op">+</span> get_2d_offset(rm, rk, stride_am, stride_ak)</span>
<span id="cb71-37"><a href="#cb71-37" aria-hidden="true" tabindex="-1"></a>    offs_b <span class="op">=</span> b_ptr <span class="op">+</span> get_2d_offset(rk, rn, stride_bk, stride_bn)</span>
<span id="cb71-38"><a href="#cb71-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize and iteratively update accumulator</span></span>
<span id="cb71-39"><a href="#cb71-39" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> tl.zeros((bm, bn), dtype<span class="op">=</span>tl.float32)</span>
<span id="cb71-40"><a href="#cb71-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k, bk):</span>
<span id="cb71-41"><a href="#cb71-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># todo umer: don't we need mask when loading a &amp; b?</span></span>
<span id="cb71-42"><a href="#cb71-42" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> tl.load(offs_a)</span>
<span id="cb71-43"><a href="#cb71-43" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> tl.load(offs_b)</span>
<span id="cb71-44"><a href="#cb71-44" aria-hidden="true" tabindex="-1"></a>        acc <span class="op">+=</span> tl.dot(a, b, allow_tf32<span class="op">=</span><span class="va">False</span>) <span class="co"># block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile</span></span>
<span id="cb71-45"><a href="#cb71-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># increase offets, so next iteration loads next chunks</span></span>
<span id="cb71-46"><a href="#cb71-46" aria-hidden="true" tabindex="-1"></a>        offs_a <span class="op">+=</span> bk <span class="op">*</span> stride_ak</span>
<span id="cb71-47"><a href="#cb71-47" aria-hidden="true" tabindex="-1"></a>        offs_b <span class="op">+=</span> bk <span class="op">*</span> stride_bk</span>
<span id="cb71-48"><a href="#cb71-48" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> c_ptr <span class="op">+</span> get_2d_offset(rm, rn, stride_cm, stride_cn)</span>
<span id="cb71-49"><a href="#cb71-49" aria-hidden="true" tabindex="-1"></a>    mask <span class="op">=</span> get_2d_mask(rm, rn, m, n)</span>
<span id="cb71-50"><a href="#cb71-50" aria-hidden="true" tabindex="-1"></a>    tl.store(c, acc, mask<span class="op">=</span>mask)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grouped_autotuned_matmul(a, b):</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn <span class="op">=</span> grouped_autotuned_matmul_k</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> a.shape[<span class="dv">1</span>] <span class="op">==</span> b.shape[<span class="dv">0</span>], <span class="st">"matrix dims not compatible for matmul"</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    check_tensors_gpu_ready(a, b)</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    (m, k), (_, n) <span class="op">=</span> a.shape, b.shape</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> torch.empty((m, n), device<span class="op">=</span>a.device, dtype<span class="op">=</span>torch.float16)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> <span class="kw">lambda</span> meta: (triton.cdiv(m, meta[<span class="st">'bm'</span>]),  triton.cdiv(n, meta[<span class="st">'bn'</span>]))</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>    matmul_k_fn[grid](</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a>        a, b, c,</span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>        m, n, k,</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>        a.stride(<span class="dv">0</span>), a.stride(<span class="dv">1</span>),</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>        b.stride(<span class="dv">0</span>), b.stride(<span class="dv">1</span>),</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>        c.stride(<span class="dv">0</span>), c.stride(<span class="dv">1</span>),</span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># bm=bs, bn=bs, bk=bs, &lt;- will be autotuned</span></span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># **group_sz &lt;- will be autotuned</span></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>a,b <span class="op">=</span> torch.ones(<span class="dv">3</span>,<span class="dv">4</span>, device<span class="op">=</span><span class="st">'cuda'</span>), torch.ones(<span class="dv">4</span>,<span class="dv">5</span>, device<span class="op">=</span><span class="st">'cuda'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>a<span class="op">@</span>b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0')</code></pre>
</div>
</div>
<p><em>Note: sometimes the following line returns wrong results, and I can’t reliably reproduce it. If you can, please tell me via Twitter (<span class="citation" data-cites="UmerHAdil">@UmerHAdil</span>) ! 🙏🏽</em></p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>grouped_autotuned_matmul(a,b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor([[4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.],
        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)</code></pre>
</div>
</div>
<p>For tips, tricks and heuristics which configs to try for auto-tuning, see <a href="https://www.youtube.com/watch?v=SGhfUhlowB4">Mark Saroufim’s talk “CUDA Performance Checklist”</a>. Much of it should apply to Triton as well.</p>
<p>Let’s run the benchmark once again. This will take a lot of time, as we auto-tune for each benchmarking paramater choice (i.e., 12-5=7 times for us).</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="at">@triton.testing.perf_report</span>(</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>    triton.testing.Benchmark(</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>        x_names<span class="op">=</span>[<span class="st">'square_matrix_size'</span>], x_vals<span class="op">=</span>[<span class="dv">2</span><span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">12</span>, <span class="dv">1</span>)], x_log<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>        line_arg<span class="op">=</span><span class="st">'provider'</span>, line_vals<span class="op">=</span>[<span class="st">'naive'</span>, <span class="st">'grouped'</span>, <span class="st">'grouped-autotuned'</span>, <span class="st">'torch'</span>], line_names<span class="op">=</span>[<span class="st">'Naive'</span>, <span class="st">'Grouped'</span>, <span class="st">'Grouped &amp; Auto-Tuned'</span>,<span class="st">'Torch'</span>],</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>        styles<span class="op">=</span>[(<span class="st">'blue'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'-'</span>), (<span class="st">'green'</span>, <span class="st">'--'</span>), (<span class="st">'orange'</span>,<span class="st">'-'</span>)],</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>        ylabel<span class="op">=</span><span class="st">'GB/s'</span>, plot_name<span class="op">=</span><span class="st">'matmul-performance'</span>, args<span class="op">=</span>{}</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> benchmark(square_matrix_size, provider):</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    sz <span class="op">=</span> square_matrix_size</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> torch.rand((sz, sz), device<span class="op">=</span><span class="st">'cuda'</span>, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    quantiles <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>]</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'naive'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: naive_matmul(a, b, bs<span class="op">=</span><span class="dv">64</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped'</span>: ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: grouped_matmul(a, b, group_sz<span class="op">=</span><span class="dv">8</span>, bs<span class="op">=</span><span class="dv">64</span>), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'grouped-autotuned'</span>: ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: grouped_autotuned_matmul(a, b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> provider <span class="op">==</span> <span class="st">'torch'</span>:   ms, min_ms, max_ms <span class="op">=</span> triton.testing.do_bench(<span class="kw">lambda</span>: torch.matmul(a,b), quantiles<span class="op">=</span>quantiles)</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    gbps <span class="op">=</span> <span class="kw">lambda</span> ms: <span class="dv">12</span> <span class="op">*</span> sz <span class="op">/</span> ms <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gbps(ms), gbps(max_ms), gbps(min_ms)</span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>benchmark.run(print_data<span class="op">=</span><span class="va">True</span>, show_plots<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="A_Practitioners_Guide_to_Triton_files/figure-html/cell-81-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>matmul-performance:
   square_matrix_size     Naive   Grouped  Grouped &amp; Auto-Tuned     Torch
0                32.0  0.040067  0.037500              0.062176  0.054795
1                64.0  0.077170  0.074303              0.091954  0.104803
2               128.0  0.110218  0.107143              0.117936  0.169912
3               256.0  0.139738  0.136364              0.137339  0.126482
4               512.0  0.083953  0.082937              0.066864  0.110983
5              1024.0  0.023112  0.025932              0.020007  0.033520
6              2048.0  0.005235  0.005912              0.004629  0.007076</code></pre>
</div>
</div>
<hr>
<h1>
That’s it! Congrats on making it through the tutorial - good work! 🥳
</h1>
<p>I highly encourage you to write a few triton kernels yourself. You can e.g.&nbsp;try these triton puzzles: https://github.com/srush/Triton-Puzzles by <a href="https://twitter.com/srush_nlp">Sasha Rush</a>, Tejas Ramesh and <a href="https://twitter.com/ZhouKeren">Keren Zhou</a>.</p>
<p>Here is other intermediate and advanced material: - The official documentation: https://triton-lang.org/ - The LightLLM repo has a ton of real-world triton kernels: https://github.com/ModelTC/lightllm/tree/main/lightllm/common/basemodel/triton_kernel - So does the Unsloth repo: https://github.com/unslothai/unsloth/tree/main/unsloth/kernels</p>
<p>If you’re generally interested in GPU programming and performance, the <a href="https://discord.gg/cudamode">cuda mode Discord</a> may be interesting to you. This tutorial was written as part of their amazing <a href="https://www.youtube.com/@CUDAMODE">lecture series</a>.</p>
<hr>
<p><strong>About the author:</strong></p>
<p>Hey 👋🏽 I’m Umer from Germany. Thanks for reading this tutorial. I hope you got learned a lot from it. If you have any questions, feel free to shoot me a message on Twitter (<a href="https://x.com/UmerHAdil"><span class="citation" data-cites="UmerHAdil">@UmerHAdil</span></a>).</p>
<p>As I currently do Open-Source AI work as an independent ML engineer, I have set up a ko-fi page for tips &amp; donations: https://ko-fi.com/umerha. Apart from this guide, I’ve contributed to HuggingFace diffusers (e.g.&nbsp;<a href="https://x.com/RisingSayak/status/1773739194474463629">shoutouts by HF</a>), LangChain <a href="https://twitter.com/search?lang=de&amp;q=(from%3ALangChainAI)%20(%40UmerHAdil)%20lang%3Aen&amp;src=typed_query">shoutouts by the team</a>), and gpt-engineer (e.g.&nbsp;<a href="https://x.com/UmerHAdil/status/1715447656527339668">this</a>).</p>
<p>If you’re a company in need of Triton and/or CUDA consulting, also shoot me a message on Twitter (<a href="https://x.com/UmerHAdil"><span class="citation" data-cites="UmerHAdil">@UmerHAdil</span></a>).</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>