[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "internal/A_Practitioners_Guide_to_Triton.html",
    "href": "internal/A_Practitioners_Guide_to_Triton.html",
    "title": "Why & when to use Triton",
    "section": "",
    "text": "A practitioner’s guide to Triton\n\nBy UmerHA (https://x.com/UmerHAdil // https://github.com/UmerHA/), for the cuda-mode group ❤️ May our brrrr level reach over 9000.\nWhat is Triton\nIn short: Triton is a language to program GPUs more conventiently. You write Python-ish code, which is then compiled into ptx code (the same thing cuda code is compiled into).\nDuring the compilation, the Triton compiler tries to use clever tricks to rearrange the parts of your program (without changing the program’s meaning!) to make it run faster.\nTriton vs Cuda\n\nsource: https://zhuanlan.zhihu.com/p/672086654\nCUDA is a high-end tool with many settings for the pros. - full control over everything, so absolute max performance possible - harder to get decent performance - way more tedious to write and debug - more complicated, so harder to learn\nTriton is a very good tool for most users - you can’t control everything, as some things are left to automatic optimization; so you probably won’t get absolute max performance - way easier to get good performance - way easier to write and debug - easier to learn, as it has a Python-like syntax\nTriton vs torch.compile\ntorch.compile makes your model faster by trying to use existing kernels more effectively and creating simple new kernels. This may make your model fast enough. If not, you can decide to invest time to write faster Triton kernels.\n(These simple new kernels that torch.compile creates are actually Triton kernels. So they are a good starting point for your custom kernels. See Mark Saroufim’s lecture 1 of cuda mode for how.)\nWhen to use Triton\nYou start with your AI model. 1. If it’s not fast enough, torch.compile it. 2. If it’s not fast enough, check if you can rewrite your code to make it more suitable for torch.compile. 3. If it’s not fast enough, check which parts are slow and write custom Triton kernel(s) for those. 4. If it’s not fast enough, check which parts are slow and write custom CUDA kernel(s) for those.\n(In the unlikely case you know beforehand tyou need absolute max performance, you can decide to directly start with CUDA.)\nA note on rough edges\nAs Triton is a newer project, people have found it to have a few rough edges. I have noted all rough edges I encountered with the comment “Weirdness: &lt;description of what’s weird to me&gt;”.\nI expect it to get a lot more polished over time.\n\nHow to write Triton kernels\nUnlike with CUDA, we can debug Triton kernels just like any CPU program, if we set the environment variable TRITON_INTERPRET = 1. Then Triton runs on the CPU but simulates that it runs on the GPU.\nI recommend writing all programs in the simulator first, and checking for correctness. If correct, then you can make it fast.\nBelow are some utility functions for debugging: - check_tensors_gpu_ready: (i) assert all tensors are contiguous in memory and (ii) only if not simulating, assert all tensors are on gpu - breakpoint_if: set a breakpoint, depending on conditions on pids - print_if print sth, depending on conditions on pids\n\nimport os\nfrom IPython.core.debugger import set_trace\n\nos.environ['TRITON_INTERPRET'] = '1' # needs to be set *before* triton is imported\n\ndef check_tensors_gpu_ready(*tensors):\n    for t in tensors:\n        assert t.is_contiguous, \"A tensor is not contiguous\"\n        if not os.environ.get('TRITON_INTERPRET') == '1': assert t.is_cuda, \"A tensor is not on cuda\"\n\ndef test_pid_conds(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n    '''Test if condition on pids are fulfilled\n    E.g.:\n        '=0'  checks that pid_0 == 0\n        ',&gt;1' checks that pid_1 &gt; 1\n        '&gt;1,=0' checks that pid_0 &gt; 1 and pid_1 == 0\n    '''\n    pids = pid_0[0], pid_1[0], pid_2[0]\n    conds = conds.replace(' ','').split(',')\n    for i, (cond, pid) in enumerate(zip(conds, pids)):\n        if cond=='': continue\n        op, threshold = cond[0], int(cond[1:])\n        if op not in ['&lt;','&gt;','&gt;=','&lt;=','=', '!=']: raise ValueError(f\"Rules may only use these ops: '&lt;','&gt;','&gt;=','&lt;=','=', '!='. Invalid rule: '{condition}'.\")\n        op = '==' if op == '=' else op\n        if not eval(f'{pid} {op} {threshold}'): return False\n    return True\n\nassert test_pid_conds('')\nassert test_pid_conds('&gt;0', [1], [1])\nassert not test_pid_conds('&gt;0', [0], [1])\nassert test_pid_conds('=0,=1', [0], [1], [0])\n\ndef breakpoint_if(conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n    '''Stop kernel, if any condition of pids is fulfilled'''\n    if test_pid_conds(conds, pid_0, pid_1, pid_2): set_trace()\n\ndef print_if(txt, conds, pid_0=[0], pid_1=[0], pid_2=[0]):\n    '''Print txt, if any condition of pids is fulfilled'''\n    if test_pid_conds(conds, pid_0, pid_1, pid_2): print(txt)\n\ndef cdiv(a,b): return (a + b - 1) // b\nassert cdiv(10,2)==5\nassert cdiv(10,3)==4\n\n\nimport torch\nimport triton\nimport triton.language as tl\n\n\n\nProgramming model\nWith CUDA, we decompose the computation in 2 levels: First into blocks, then each block further into threads. All threads in a block run on the same SM and share the same Shared Memory. And each thread computes on scalars.\nIn Triton, we decompose the computation only in 1 level: Into blocks. There is no further decomposition into threads. Triton requires us to perform operations on vectors. Also, we don’t need to and are not able to manage the shared memory. Triton does that automatically.\nExample:\nLet’s say we want to add x and y, which are vectors of size 8, and save the output into z (also size 8). Let’s use blocks of size 4, so we have 8 / 4 = 2 blocks. - Cuda runs 2 blocks, each with 4 threads. Each of the 8 threads computes a single position, e.g. z[0] = x[0] + y[0] - Triton also runs 2 blocks, which each performs vectorized addition. The vector size is the block size, which is 4. E.g. z[0:3] = x[0:3] + y[0:3]\nAll operations in triton kernels are vectorized: Loading data, operating on data, storing data, and creating masks.\nLet’s think about another simple example:\nAgain, we want to add x and y, which are now vectors of size 6, and save the output into z (also size 6). Let’s use blocks of size 4, so we have cdiv(6, 4) = 2 blocks.\n\nx = torch.tensor([1,2,3,4,5,6])\ny = torch.tensor([0,1,0,1,0,1])\n\nx, y, x+y\n\n(tensor([1, 2, 3, 4, 5, 6]),\n tensor([0, 1, 0, 1, 0, 1]),\n tensor([1, 3, 3, 5, 5, 7]))\n\n\nThe cuda kernel would be the C-equivalent of this:\n\n# x,y = input tensors, z = output tensors, n = size of x, bs = block size\ndef add_cuda_k(x, y, z, n, bs):\n    # locate which part of the overall computation this specific kernel is doing\n    block_id = ... # in our example: one of [0,1] \n    thread_id = ... # in our example: one of [0,1,2,3] \n\n    # identify the location of the data this specific kernel needs\n    offs = block_id * bs + thread_id\n    \n    # guard clause, to make sure we're not going out of bounds\n    if offs &lt; n:\n\n        # read data\n        x_value = x[offs]\n        y_value = y[offs]\n        \n        # do operation\n        z_value = x_value + y_value\n        \n        # write data\n        z[offs] = z_value\n\n    # Important: offs, x_value, y_value, x_value are all scalars!\n    # The guard condition kind of is also a scalar, as it check one condition on one value.\n\nFor illustration, here are the variables for every kernel:\n\nLet’s now look at the corresponding triton kernel, which roughly looks like this:\n\n# Note: this is for illustration, and not quite syntactically correct. See further below for correct triton syntax\n\ndef add_triton_k(x, y, z, n, bs):\n    # locate which part of the overall computation this specific kernel is doing\n    block_id = tl.program_id(0)  # in our example: one of [0,1] \n    \n    # identify the location of the data this specific kernel needs\n    offs = block_id * bs + tl.arange(0, bs) # &lt;- this is a vector!\n    \n    # the guard clause becomes a mask, which is a vector of bools\n    mask = offs &lt; n # &lt;- this is a vector of bools!\n    \n    # read data\n    x_values = x[offs] # &lt;- a vector is read!\n    y_values = y[offs] # &lt;- a vector is read!\n    \n    # do operation\n    z_value = x_value + y_value  # &lt;- vectors are added!\n    \n    # write data\n    z[offs] = z_value  # &lt;- a vector is written!\n\nAgain, for illustration, here are the variables for every kernel:\n\nNote on jargon: In triton lingo, each kernel (which processes a block) is called a “program”. I.e., our example above runs 2 programs. Therefore, “block_id” is often called “pid” (short for “program id”), but it’s the same.\n\n\nExample 1: Copying a tensor\nLet’s looks at some examples. To keeps things simple, we’ll use very small block sizes.\nGoal: Given a tensor x of shape (n), copy it into another tensor z.\n\n# # This is a normal python function, which launches the triton kernels\ndef copy(x, bs, kernel_fn):\n    z = torch.zeros_like(x)\n    check_tensors_gpu_ready(x, z)\n    n = x.numel()\n    n_blocks = cdiv(n, bs)\n    grid = (n_blocks,)  # how many blocks do we have? can be 1d/2d/3d-tuple or function returning 1d/2d/3d-tuple\n\n    # launch grid!\n    # - kernel_fn is the triton kernel, which we write below\n    # - grid is the grid we constructed above\n    # - x,z,n,bs are paramters that are passed into each kernel function\n    kernel_fn[grid](x,z,n,bs)\n\n    return z    \n\nNote: For educational purposes, the kernel below has a logic bug (but the syntax is correct). Can you spot it?\n\n# # This is the triton kernel:\n\n# The triton.jit decorator takes a python function and turns it into a triton kernel, which is run on the GPU.\n# Inside this function only a subset of all python ops are allowed.\n# E.g., when NOT simulating, we can't print or use breakpoints, as these don't exist on the GPU. \n@triton.jit\n# When we pass torch tensors, they are automatically converted into a pointer to their first value\n# E.g., above we passed x, but here we receive x_ptr\ndef copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = tl.arange(0, bs)  # compute the offsets from the pid \n    mask = offs &lt; n\n    x = tl.load(x_ptr + offs, mask) # load a vector of values, think of `x_ptr + offs` as `x_ptr[offs]`\n    tl.store(z_ptr + offs, x, mask) # store a vector of values\n\n    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')\n\n    # Question: What is wrong with this kernel?\n\n\nz = copy(x, bs=2, kernel_fn=copy_k)\n\npid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]\npid = [1] | offs = [0 1], mask = [ True  True], x = [1 2]\npid = [2] | offs = [0 1], mask = [ True  True], x = [1 2]\n\n\n\nz\n\ntensor([1, 2, 0, 0, 0, 0])\n\n\nWe were not shifting the offets correcltly. We always used offsets = [0,1], but they should change with the pid.\n\n@triton.jit\ndef copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * n + tl.arange(0, bs)\n    mask = offs &lt; n\n    x = tl.load(x_ptr + offs, mask)\n    tl.store(z_ptr + offs, x, mask)\n    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')\n\n\nz = copy(x, bs=2, kernel_fn=copy_k)\n\npid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]\npid = [1] | offs = [6 7], mask = [False False], x = [1 1]\npid = [2] | offs = [12 13], mask = [False False], x = [1 1]\n\n\nNot quite correct. We added pid * n, but want to add pid * bs\n\n@triton.jit\ndef copy_k(x_ptr, z_ptr, n, bs: tl.constexpr):\n    pid = tl.program_id(0)\n    offs = pid * bs + tl.arange(0, bs)\n    mask = offs &lt; n\n    x = tl.load(x_ptr + offs, mask)\n    tl.store(z_ptr + offs, x, mask)\n    print_if(f'pid = {pid} | offs = {offs}, mask = {mask}, x = {x}', '')\n\n\nz = copy(x, bs=2, kernel_fn=copy_k)\n\npid = [0] | offs = [0 1], mask = [ True  True], x = [1 2]\npid = [1] | offs = [2 3], mask = [ True  True], x = [3 4]\npid = [2] | offs = [4 5], mask = [ True  True], x = [5 6]\n\n\nYes!\n\nx, z\n\n(tensor([1, 2, 3, 4, 5, 6]), tensor([1, 2, 3, 4, 5, 6]))\n\n\nAs we saw, writing GPU programs involves many indices, which we can easily mess up. So I highly recommend writing and debugging the kernel in simuation mode, and testing with tiny examples first!\n\n\nExample 2: Greyscaling an image\nRestart kernel here\nIn this example, we’ll grayscale an image of a puppy. We’ll see how we can work on 2d data.\nThis works analogously for 3D data.\nWe’ve adapted Jeremy Howard’s example from this colab / youtube. So, h/t for the example and selection of puppy image.\nSide note: Two weird things happen in this example, if we don’t restart the kernel: 1. torchvision can’t be imported, probably due to a circular dependency. -&gt; I currently don’t know why, need to dig deeper. 2. the simulated triton kernel below fails, because a float can’t be mutliplied to a uint vector -&gt; Works on GPU w/o simulation, so seems to be a TRITON_INTERPRET bug.\n\nimport os\n\nimport matplotlib.pyplot as plt\nfrom urllib.request import urlretrieve\nfrom pathlib import Path\n\nimport torch\nfrom torch import tensor\nimport torchvision as tv\nimport torchvision.transforms.functional as tvf\nfrom torchvision import io\n\nimport triton\nimport triton.language as tl\n\n\ndef cdiv(a,b): return (a + b - 1) // b\n\n\nurl = 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Cute_dog.jpg/1600px-Cute_dog.jpg?20140729055059'\n\n\npath_img = Path('puppy.jpg')\nif not path_img.exists(): urlretrieve(url, path_img)\n\n\nimg = io.read_image('puppy.jpg')\nprint(img.shape)\nimg[:2,:3,:4]\n\ntorch.Size([3, 1066, 1600])\n\n\ntensor([[[117, 119, 117, 113],\n         [119, 129, 129, 113],\n         [130, 126, 122, 115]],\n\n        [[ 83,  85,  85,  80],\n         [ 85,  97,  97,  82],\n         [ 98,  93,  89,  83]]], dtype=torch.uint8)\n\n\n\ndef show_img(x, figsize=(4,3), **kwargs):\n    plt.figure(figsize=figsize)\n    plt.axis('off')\n    if len(x.shape)==3: x = x.permute(1,2,0)  # CHW -&gt; HWC\n    plt.imshow(x.cpu(), **kwargs)\n\n\nimg = tvf.resize(img, 150, antialias=True)\nch,h,w = img.shape\nch,h,w,h*w\n\n(3, 150, 225, 33750)\n\n\n\nshow_img(img)\n\n\n\n\nTo work with 2d data, we’ll build 2d offsets and masks. Here’s an illustration how it works, e.g. for an 4x7 matrix and block sizes of 2 for each dimensions.\n\nAnd in code, it looks like this:\n\n@triton.jit\ndef rgb2grey_k(x_ptr, out_ptr, h, w, bs0: tl.constexpr, bs1: tl.constexpr):\n    pid_0 = tl.program_id(0)\n    pid_1 = tl.program_id(1)\n    \n    offs_0 = pid_0 * bs0 + tl.arange(0,bs0)  # 1d vector\n    offs_1 = pid_1 * bs1 + tl.arange(0,bs1)  # 1d vector\n\n    # Weirdness: None-slicing currently doesn't work when simulating on cpu. Use tl.expand_dim instead.\n    # offs = w * tl.expand_dims(offs_0, 1) + tl.expand_dims(offs_1, 0)\n    offs = w * offs_0[:,None] + offs_1[None, :]  # 2d matrix! - we multiply first offset by width, see image above\n\n    mask_0 = offs_0 &lt; h  # 1d vector\n    mask_1 = offs_1 &lt; w  # 1d vector\n\n    # mask = tl.expand_dims(mask_0, 1) & tl.expand_dims(mask_1, 0)\n    mask = mask_0[:,None] & mask_1[None,:]  # 2d matrix! - data musn't go out of bounds along either axis, therefore `logical and` of the individual masks\n    \n    r = tl.load(x_ptr + 0*h*w+offs, mask=mask)\n    g = tl.load(x_ptr + 1*h*w+offs, mask=mask)\n    b = tl.load(x_ptr + 2*h*w+offs, mask=mask)\n\n    # Weirdness: multiplying float with uint vectors fails when simulating on cpu\n    out = 0.2989*r + 0.5870*g + 0.1140*b  # don't worry why it's these 3 numbers we're multiplying with\n\n    tl.store(out_ptr + offs, out, mask=mask)\n\nLet’s use the kernel!\n\ndef rgb2grey(x, bs):\n    c,h,w = x.shape\n    out = torch.empty((h,w), dtype=x.dtype, device=x.device)\n\n    # grid can be a function returning a 1d/2d/3d-tuple\n    # (having a grid function is not more useful than a grid tuple in this case, but will be below when benchmarking & auto-tuning)\n    grid = lambda meta: (cdiv(h, meta['bs0']), cdiv(w,  meta['bs1']))\n    \n    rgb2grey_k[grid](x, out, h, w, bs0=bs[0], bs1=bs[1]) # all kwargs are passed into grid function\n    return out.view(h,w)\n\n\ngrey_img = rgb2grey(img.to('cuda'), bs=(32, 32)).to('cpu')\n\n\nshow_img(grey_img, cmap='gray')\n\n\n\n\nVery cool\n\n\nExample 3: Matmul\nFor simplicity, restart kernel here\n\nimport os\n# os.environ['TRITON_INTERPRET'] = '1'\n\nimport torch\nimport triton\nimport triton.language as tl\n\n# moved util functions into separate file for better readability\nfrom triton_util import cdiv, breakpoint_if, print_if, check_tensors_gpu_ready\n\nNow, let’s implement a naive matmul in Triton. We’ll learn: - A method to split computation - Calling functions from our kernel - Using pre-implemented vector/matrix ops within an block\nThis is adapted from the OpenAI blog post announcing Triton.\nWe want to multiply the m x k-matrix A and the k x n-matrix B into the m x n-matrix C.\nWe split the computation along each of the three axes: - along the m axis - we’ll use block dimension 0 to represent this - along the n axis - we’ll use block dimension 1 to represent this - along the shared k axis - this will not be represented by a block. All chunks of computation will be done in same block.\n\nBecause we frequently create 1d- or 2d-offets and -masks, let’s put that functionality into utility functions. As long as these functions are triton.jit-ed, they can be used in the kernel.\n\n@triton.jit\ndef get_1d_offset(size, n_prev_chunks):\n    return n_prev_chunks * size + tl.arange(0, size)\n\n@triton.jit\ndef get_2d_offset(offs_0, offs_1, stride_0, stride_1=1): \n    return tl.expand_dims(offs_0, 1)*stride_0 + tl.expand_dims(offs_1, 0)*stride_1\n\n@triton.jit\ndef get_1d_mask(offs, max):\n    return offs &lt; max\n\n@triton.jit\ndef get_2d_mask(offs_0, offs_1, max_0, max_1):\n    return (tl.expand_dims(offs_0, 1) &lt; max_0) & (tl.expand_dims(offs_1, 0) &lt; max_1)\n\nHere’s the naive matmul kernel:\n\n@triton.jit\ndef naive_matmul_k(\n    a_ptr, b_ptr, c_ptr,\n    m, n, k,\n    stride_am, stride_ak, \n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr\n):\n    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n    # chunks along m/n/k dimensions\n    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)\n    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)\n    rk = get_1d_offset(size=bk, n_prev_chunks=0)\n    # relevant offsets of a, b\n    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n    # initialize and iteratively update accumulator\n    acc = tl.zeros((bm, bn), dtype=tl.float32)\n    for _ in range(0, k, bk):\n        # todo umer: don't we need mask when loading a & b?\n        a = tl.load(offs_a)\n        b = tl.load(offs_b)\n        acc += tl.dot(a, b, allow_tf32=False) # matmul in block ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n        # increase offets, so next iteration loads next chunks\n        offs_a += bk * stride_ak\n        offs_b += bk * stride_bk\n    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n    mask = get_2d_mask(rm, rn, m, n)\n    tl.store(c, acc, mask=mask)\n\n\nfrom functools import partial\n\ndef matmul(a, b, matmul_k_fn, bs=16, group_sz=None):\n    assert a.shape[1] == b.shape[0], \"matrix dims not compatible for matmul\"\n    check_tensors_gpu_ready(a, b)\n    (m, k), (_, n) = a.shape, b.shape\n    c = torch.empty((m, n), device=a.device, dtype=torch.float16)\n    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))\n    group_sz = {} if group_sz is None else {\"group_sz\":group_sz} # not used in naive_matmul, but will be in grouped_matmul further below \n    matmul_k_fn[grid](\n        a, b, c,\n        m, n, k,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        bm=bs, bn=bs, bk=bs, # Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n        **group_sz\n    )\n    return c\n\nnaive_matmul = partial(matmul, matmul_k_fn=naive_matmul_k)\n\n\na = torch.ones((3, 4), dtype=torch.float32, device='cuda')\nb = torch.ones((4, 5), dtype=torch.float32, device='cuda')\n\n\nnaive_matmul(a,b)\n\ntensor([[4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)\n\n\nLet’s unit test this against PyTorch’s implementation\n\ntorch.manual_seed(0)\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\ntriton_output = naive_matmul(a, b)\ntorch_output = torch.matmul(a, b)\nif torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):\n    print(\"✅ Triton and Torch match\")\nelse:\n    print(\"❌ Triton and Torch differ\")\n\n✅ Triton and Torch match\n\n\n\n\nExample 4: Faster Matmul\nNote: Needs code from example 3, so run that before\nTriton handles the order of memory access within blocks, but not across blocks. So this is a knob we can use to make our kernels faster.\nIn fact, cleverly reordering blocks can increase L2-cache hit rate, which makes our kernels faster. This example is taken from the triton docs.\nNow, to make better use of the L2 cache, we want to reuse data that’s was recently loaded, and is therefore likely still in the L2 cache. How? By reducing the number of different data loads that a bunch of “consecutive” kernels need. By “consecutive” we mean kernels that are executed approximately at the same time.\nThis picture (adapter from the triton docs) shows how we can do that. If we order naively, the first row of the output matrix is computed “consecutively”, which needs 90 different block reads (9 from matrix A, 81 from matrix B). If we use “group ordering”, a 3x3 square of blocks of the output matrix is computed “consecutively”, which needs 54 different block reads (27 from matrix A, 27 from matrix B).\n\nNote: In the docs, grouping is called “super-grouping”\nOkay, how can we tell Triton in which order to process blocks? The answer is: We take the pids, change them, and use them as if they were the original pids.\nLet’s do a minimal example to illustrate this principle:\n\ndef process_item(id): print(f\"I'm processing item {id}\")\n\nfor i in range(5): process_item(i)\n\nI'm processing item 0\nI'm processing item 1\nI'm processing item 2\nI'm processing item 3\nI'm processing item 4\n\n\n\ndef change_id(old_id): return 5-old_id\n\nfor i in range(5): process_item(change_id(i))\n\nI'm processing item 5\nI'm processing item 4\nI'm processing item 3\nI'm processing item 2\nI'm processing item 1\n\n\nEt voilà, the items were processed in a different order.\nSo how should the pid-change-function for faster matmul look like? It should change the left matrix into the right matrix.\n\nOn the left, the default ordering is shown (called “row-major”). Remember, we deal with blocks. We can’t arrange how the individual cells are processed, only the blocks. In the picture, our output matrix C has 5x7 = 35 cells, but only cdiv(5,1) x cdiv(7,2) = 5x4 = 20 blocks.\nOn the right, notice how the first 9 processed blocks are the 3x3 grid we want! We process 3 blocks in a column. Then advance a column, again process 3, advance, and so on. The orange lines show where advance. This operation is called “swizzling”.\nBy the way, you can of course change the number 3. It’s called the group_size.\nYou don’t need to write swizzling yourself, as there is a triton.language.swizzle2d function.\nTo really understand swizzle2d, let’s quickly verifiy it works as expected. We’ll then continue to use it in our faster matmul kernel.\nSide-Goal: Use swizzle2d on a 5x4 matrix with elements 0 ... 19 in row-major ordering. We should then get a matrix with elements in grouped ordering.\n\n@triton.jit\ndef swizzle_k(x_ptr, z_ptr, group_sz: tl.constexpr):\n    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n\n    pid_m_, pid_n_ = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n    \n    offs_m = get_1d_offset(1, n_prev_chunks=pid_m)\n    offs_n = get_1d_offset(1, n_prev_chunks=pid_n)\n    \n    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)\n    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n )\n\n    offs_sw_m = get_1d_offset(1, n_prev_chunks=pid_m_)\n    offs_sw_n = get_1d_offset(1, n_prev_chunks=pid_n_)\n    \n    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)\n    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)\n    \n    x = tl.load(x_ptr + offs, mask=mask)\n    tl.store(z_ptr + offs_sw, x, mask=mask_sw)\n\n\nblocks_m, blocks_n = 5,4\n\nx = torch.arange(blocks_m*blocks_n, device='cuda').view(blocks_m,blocks_n)\nx\n\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19]], device='cuda:0')\n\n\n\nz = -torch.ones_like(x) # empty matrix, with -1 denoting empty\nz\n\ntensor([[-1, -1, -1, -1],\n        [-1, -1, -1, -1],\n        [-1, -1, -1, -1],\n        [-1, -1, -1, -1],\n        [-1, -1, -1, -1]], device='cuda:0')\n\n\n\n# swizzle x into z\nswizzle_k[(blocks_m,blocks_n)](x,z, group_sz=3);\n\n\nz\n\ntensor([[ 0,  3,  6,  9],\n        [ 1,  4,  7, 10],\n        [ 2,  5,  8, 11],\n        [12, 14, 16, 18],\n        [13, 15, 17, 19]], device='cuda:0')\n\n\nLooks good!\n\nLet’s now implement the grouped matmul kernel, which will be faster than the regular matmul.\n\n@triton.jit\ndef grouped_matmul_k(\n    a_ptr, b_ptr, c_ptr,\n    m, n, k,\n    stride_am, stride_ak, \n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr\n):\n    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n    # determine location of block in grouped ordering - swizzle! \n    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n    # chunks along m/n/k dimensions\n    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)\n    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)\n    rk = get_1d_offset(size=bk, n_prev_chunks=0)\n    # relevant offsets of a, b\n    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n    # initialize and iteratively update accumulator\n    acc = tl.zeros((bm, bn), dtype=tl.float32)\n    for _ in range(0, k, bk):\n        # todo umer: don't we need mask when loading a & b?\n        a = tl.load(offs_a)\n        b = tl.load(offs_b)\n        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n        # increase offets, so next iteration loads next chunks\n        offs_a += bk * stride_ak\n        offs_b += bk * stride_bk\n    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n    mask = get_2d_mask(rm, rn, m, n)\n    tl.store(c, acc, mask=mask)\n\n\ngrouped_matmul = partial(matmul, matmul_k_fn=grouped_matmul_k)\n\n\na = torch.ones((3, 4), dtype=torch.float32, device='cuda')\nb = torch.ones((4, 5), dtype=torch.float32, device='cuda')\n\n\ngrouped_matmul(a,b, group_sz=4)\n\ntensor([[4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)\n\n\nLet’s unit test this against PyTorch’s implementation\n\ntorch.manual_seed(0)\na = torch.randn((512, 512), device='cuda', dtype=torch.float16)\nb = torch.randn((512, 512), device='cuda', dtype=torch.float16)\ntriton_output = grouped_matmul(a, b, group_sz=32)\ntorch_output = torch.matmul(a, b)\nif torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):\n    print(\"✅ Triton and Torch match\")\nelse:\n    print(\"❌ Triton and Torch differ\")\n\n✅ Triton and Torch match\n\n\n\n\nBenchmarking\nTriton brings built-in benchmarking tools with it. Here’s an example how to use it.\n\n# adapted from https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        x_names=['square_matrix_size'],  # Argument names to use as an x-axis for the plot.\n        x_vals=[2**i for i in range(5, 12, 1)],  # Different possible values for `x_name`.\n        x_log=True,  # x axis is logarithmic.\n        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n        line_vals=['naive', 'grouped', 'torch'],  # Possible values for `line_arg`.\n        line_names=['Naive', 'Grouped', 'Torch'],  # Label name for the lines.\n        styles=[('blue', '-'), ('green', '-'), ('orange','-')],  # Line styles.\n        ylabel='GB/s',  # Label name for the y-axis.\n        plot_name='matmul-performance',  # Name for the plot. Used also as a file name for saving the plot.\n        args={},  # Values for function arguments not in `x_names` and `y_name`.\n    ))\ndef benchmark(square_matrix_size, provider):\n    sz = square_matrix_size\n    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b), quantiles=quantiles)\n    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8), quantiles=quantiles)\n    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)\n    gbps = lambda ms: 12 * sz / ms * 1e-6\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\n\n\nbenchmark.run(print_data=True, show_plots=True)\n\n\n\n\nmatmul-performance:\n   square_matrix_size     Naive   Grouped     Torch\n0                32.0  0.085106  0.085106  0.053691\n1                64.0  0.129730  0.125000  0.107143\n2               128.0  0.159468  0.154341  0.170515\n3               256.0  0.097909  0.099071  0.125654\n4               512.0  0.030346  0.030361  0.111079\n5              1024.0  0.006971  0.007279  0.034461\n6              2048.0  0.001405  0.001749  0.006355\n\n\nNote Umer: I would’ve expected the GB/s to increase as the matrix sizes get larger. Why don’t they? Maybe because share memory is full, so kernel spends more and more time reloading stuff\nLet’s try different block sizes:\n\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        x_names=['batch_size'], x_vals=[2**i for i in range(4, 7, 1)], x_log=True,\n        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],\n        styles=[('blue', '-'), ('green', '-'), ('orange','-')],\n        ylabel='GB/s', plot_name='matmul-performance', args={}\n    ))\ndef benchmark(batch_size, provider):\n    sz = 512\n    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=batch_size), quantiles=quantiles)\n    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, bs=batch_size, group_sz=8), quantiles=quantiles)\n    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)\n    gbps = lambda ms: 12 * sz / ms * 1e-6\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\n\nbenchmark.run(print_data=True, show_plots=True)\n\n\n\n\nmatmul-performance:\n   batch_size     Naive   Grouped     Torch\n0        16.0  0.030404  0.030433  0.111111\n1        32.0  0.060683  0.061127  0.111111\n2        64.0  0.083660  0.084026  0.111111\n\n\nLarger block sizes seem to be better. Let’s compare with pytorch again, using larger block sizes.\n\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,\n        line_arg='provider', line_vals=['naive', 'grouped', 'torch'], line_names=['Naive', 'Grouped', 'Torch'],\n        styles=[('blue', '-'), ('green', '-'), ('orange','-')],\n        ylabel='GB/s', plot_name='matmul-performance', args={}\n    ))\ndef benchmark(square_matrix_size, provider):\n    sz = square_matrix_size\n    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)\n    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)\n    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)\n    gbps = lambda ms: 12 * sz / ms * 1e-6\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\n\nbenchmark.run(print_data=True, show_plots=True)\n\n\n\n\nmatmul-performance:\n   square_matrix_size     Naive   Grouped     Torch\n0                32.0  0.039867  0.038710  0.053215\n1                64.0  0.077922  0.071006  0.106667\n2               128.0  0.109091  0.107143  0.169912\n3               256.0  0.137733  0.136364  0.126150\n4               512.0  0.084731  0.083916  0.111047\n5              1024.0  0.021879  0.025362  0.034691\n6              2048.0  0.005257  0.005919  0.007440\n\n\nThis reduces the performance difference to pytorch for larger matrix sizes, but pytorch is still better.\nTip: For profiling, we can use Nsight Compute to profile our kernels: ncu --target-processes all your_python_file.py\n\n\nAuto-Tuning\nAdapted from https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html\nThe choice of meta-parameters (e.g. block sizes) and compilation options (e.g. num_warps) impacts the kernel speed. Triton allows you to pass a list of possible choices, runs them all, and then compiles the kernel for the fastest choice. This is called Auto-Tuning.\nIf the size of your problem changes (e.g. when matrix changes size), a new auto-tune will be done for the new problem size.\n\n@triton.autotune(\n    # Choices of configs to auto-tune over\n    configs=[\n        triton.Config({'bm': 128, 'bn': 256, 'bk': 64, 'group_sz': 8}, num_stages=3, num_warps=8),\n        triton.Config({'bm': 64, 'bn': 256, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n        triton.Config({'bm': 128, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n        triton.Config({'bm': 128, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n        triton.Config({'bm': 64, 'bn': 128, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n        triton.Config({'bm': 128, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n        triton.Config({'bm': 64, 'bn': 32, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),\n        triton.Config({'bm': 32, 'bn': 64, 'bk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),\n    ],\n    # Definition of problem size. If it changes, a new auto-tune is run for the new problem size.\n    key=['m', 'n', 'k'],\n)\n@triton.jit\ndef grouped_autotuned_matmul_k(\n    a_ptr, b_ptr, c_ptr,\n    m, n, k,\n    stride_am, stride_ak, \n    stride_bk, stride_bn,\n    stride_cm, stride_cn,\n    bm: tl.constexpr, bn: tl.constexpr, bk: tl.constexpr, group_sz: tl.constexpr\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    num_pid_m = tl.num_programs(0)\n    num_pid_n = tl.num_programs(1)\n    # determine location of block in grouped ordering\n    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_sz)  # Weirdness: tl.swizzle2d doesn't work when simulating on CPU\n    # chunks along m/n/k dimensions\n    rm = get_1d_offset(size=bm, n_prev_chunks=pid_m)\n    rn = get_1d_offset(size=bn, n_prev_chunks=pid_n)\n    rk = get_1d_offset(size=bk, n_prev_chunks=0)\n    # relevant offsets of a, b\n    offs_a = a_ptr + get_2d_offset(rm, rk, stride_am, stride_ak)\n    offs_b = b_ptr + get_2d_offset(rk, rn, stride_bk, stride_bn)\n    # initialize and iteratively update accumulator\n    acc = tl.zeros((bm, bn), dtype=tl.float32)\n    for _ in range(0, k, bk):\n        # todo umer: don't we need mask when loading a & b?\n        a = tl.load(offs_a)\n        b = tl.load(offs_b)\n        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication ; Weirdness: allow_tf32 must be set to False for older GPUs, otherwise won't compile\n        # increase offets, so next iteration loads next chunks\n        offs_a += bk * stride_ak\n        offs_b += bk * stride_bk\n    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n    mask = get_2d_mask(rm, rn, m, n)\n    tl.store(c, acc, mask=mask)\n\n\ndef grouped_autotuned_matmul(a, b):\n    matmul_k_fn = grouped_autotuned_matmul_k\n    \n    assert a.shape[1] == b.shape[0], \"matrix dims not compatible for matmul\"\n    check_tensors_gpu_ready(a, b)\n    (m, k), (_, n) = a.shape, b.shape\n    c = torch.empty((m, n), device=a.device, dtype=torch.float16)\n    grid = lambda meta: (triton.cdiv(m, meta['bm']),  triton.cdiv(n, meta['bn']))\n    matmul_k_fn[grid](\n        a, b, c,\n        m, n, k,\n        a.stride(0), a.stride(1),\n        b.stride(0), b.stride(1),\n        c.stride(0), c.stride(1),\n        # bm=bs, bn=bs, bk=bs, &lt;- will be autotuned\n        # **group_sz &lt;- will be autotuned\n    )\n    return c\n\n\na,b = torch.ones(3,4, device='cuda'), torch.ones(4,5, device='cuda')\n\n\na@b\n\ntensor([[4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.]], device='cuda:0')\n\n\nNote: sometimes the following line returns wrong results, and I can’t reliably reproduce it. If you can, please tell me via Twitter (@UmerHAdil) ! 🙏🏽\n\ngrouped_autotuned_matmul(a,b)\n\ntensor([[4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4.]], device='cuda:0', dtype=torch.float16)\n\n\nFor tips, tricks and heuristics which configs to try for auto-tuning, see Mark Saroufim’s talk “CUDA Performance Checklist”. Much of it should apply to Triton as well.\nLet’s run the benchmark once again. This will take a lot of time, as we auto-tune for each benchmarking paramater choice (i.e., 12-5=7 times for us).\n\n@triton.testing.perf_report(\n    triton.testing.Benchmark(\n        x_names=['square_matrix_size'], x_vals=[2**i for i in range(5, 12, 1)], x_log=True,\n        line_arg='provider', line_vals=['naive', 'grouped', 'grouped-autotuned', 'torch'], line_names=['Naive', 'Grouped', 'Grouped & Auto-Tuned','Torch'],\n        styles=[('blue', '-'), ('green', '-'), ('green', '--'), ('orange','-')],\n        ylabel='GB/s', plot_name='matmul-performance', args={}\n    ))\ndef benchmark(square_matrix_size, provider):\n    sz = square_matrix_size\n    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n    quantiles = [0.5, 0.2, 0.8]\n    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b, bs=64), quantiles=quantiles)\n    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_sz=8, bs=64), quantiles=quantiles)\n    if provider == 'grouped-autotuned': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_autotuned_matmul(a, b), quantiles=quantiles)\n    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)\n    gbps = lambda ms: 12 * sz / ms * 1e-6\n    return gbps(ms), gbps(max_ms), gbps(min_ms)\n\nbenchmark.run(print_data=True, show_plots=True)\n\n\n\n\nmatmul-performance:\n   square_matrix_size     Naive   Grouped  Grouped & Auto-Tuned     Torch\n0                32.0  0.040067  0.037500              0.062176  0.054795\n1                64.0  0.077170  0.074303              0.091954  0.104803\n2               128.0  0.110218  0.107143              0.117936  0.169912\n3               256.0  0.139738  0.136364              0.137339  0.126482\n4               512.0  0.083953  0.082937              0.066864  0.110983\n5              1024.0  0.023112  0.025932              0.020007  0.033520\n6              2048.0  0.005235  0.005912              0.004629  0.007076\n\n\n\n\nThat’s it! Congrats on making it through the tutorial - good work! 🥳\n\nI highly encourage you to write a few triton kernels yourself. You can e.g. try these triton puzzles: https://github.com/srush/Triton-Puzzles by Sasha Rush, Tejas Ramesh and Keren Zhou.\nHere is other intermediate and advanced material: - The official documentation: https://triton-lang.org/ - The LightLLM repo has a ton of real-world triton kernels: https://github.com/ModelTC/lightllm/tree/main/lightllm/common/basemodel/triton_kernel - So does the Unsloth repo: https://github.com/unslothai/unsloth/tree/main/unsloth/kernels\nIf you’re generally interested in GPU programming and performance, the cuda mode Discord may be interesting to you. This tutorial was written as part of their amazing lecture series.\n\nAbout the author:\nHey 👋🏽 I’m Umer from Germany. Thanks for reading this tutorial. I hope you got learned a lot from it. If you have any questions, feel free to shoot me a message on Twitter (@UmerHAdil).\nAs I currently do Open-Source AI work as an independent ML engineer, I have set up a ko-fi page for tips & donations: https://ko-fi.com/umerha. Apart from this guide, I’ve contributed to HuggingFace diffusers (e.g. shoutouts by HF), LangChain shoutouts by the team), and gpt-engineer (e.g. this).\nIf you’re a company in need of Triton and/or CUDA consulting, also shoot me a message on Twitter (@UmerHAdil)."
  },
  {
    "objectID": "posts/gpu-puzzles-with-triton/index.html",
    "href": "posts/gpu-puzzles-with-triton/index.html",
    "title": "GPU Puzzles with Triton",
    "section": "",
    "text": "This post implemented the solutions for GPU Puzzles (which used numba) with Triton.\nWhy Triton but not CUDA? A simple reason is that I only have a Macbook. To learn GPU programming, it’s usually best to have a GPU that support CUDA. With triton, you can still implement kernel and check for correctness with just CPU. triton is a higher level “Block-based” programming than CUDA."
  },
  {
    "objectID": "posts/gpu-puzzles-with-triton/gpu-puzzles-triton.html",
    "href": "posts/gpu-puzzles-with-triton/gpu-puzzles-triton.html",
    "title": "GPU Puzzles with Triton",
    "section": "",
    "text": "This post implemented the solutions for GPU Puzzles (which used numba) with Triton.\nWhy Triton but not CUDA? A simple reason is that I only have a Macbook. To learn GPU programming, it’s usually best to have a GPU that support CUDA. With triton, you can still implement kernel and check for correctness with just CPU. triton is a higher level “Block-based” programming than CUDA."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gpu-python",
    "section": "",
    "text": "Resource for learning GPU Programming in Python\n\n\n\n\n\n\n\n\n  \n\n\n\n\nGPU Puzzles with Triton\n\n\n\n\n\n\n\ntriton\n\n\ngpu\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n\nNok\n\n\n\n\n\n\n  \n\n\n\n\nGPU Puzzles with Triton\n\n\n\n\n\n\n\ntriton\n\n\ngpu\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n\nNok\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n\nNok\n\n\n\n\n\n\nNo matching items"
  }
]